{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b2b002-326c-44b1-89a3-8e1c547cf05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "import scipy.spatial\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "552800d8-0474-4c73-a31e-6843fc7300a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(X, Xprime, gamma=2):\n",
    "    dists = scipy.spatial.distance.cdist(X, Xprime, metric='sqeuclidean')\n",
    "    \n",
    "    return np.exp(-gamma * dists)\n",
    "\n",
    "def special_kernel(X, Xprime, eta):\n",
    "    a = eta[0]\n",
    "    b = eta[1]\n",
    "    K = (1+X@Xprime.T)**2 + a * np.multiply.outer(np.sin(2*np.pi*X.reshape(-1)+b),np.sin(2*np.pi*Xprime.reshape(-1)+b))\n",
    "    \n",
    "    return K\n",
    "\n",
    "def k_rbf(x, y, l):\n",
    "    return np.exp(-(x-y)**2/(2*l**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f1d389fb-ec52-4664-a6d6-c6e0527263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and normalize Mauna Loa data \n",
    "data = np.genfromtxt('co2_mm_mlo.csv', delimiter=',')\n",
    "\n",
    "#10 years of data for learning\n",
    "X = data[:120,2]-1958\n",
    "y_raw = data[:120,3]\n",
    "y_mean = np.mean(y_raw)\n",
    "y_std = np.sqrt(np.var(y_raw))\n",
    "y = (y_raw-y_mean)/y_std\n",
    "\n",
    "#the next 5 years for prediction\n",
    "X_predict = data[120:180,2]-1958\n",
    "y_predict = data[120:180,3]\n",
    "\n",
    "# Reshape\n",
    "#X = X.reshape((1, -1))\n",
    "#y_raw = y_raw.reshape((1, -1))\n",
    "#y = y.reshape((1, -1))\n",
    "#X_predict = X_predict.reshape((1, -1))\n",
    "#y_predict = y_predict.reshape((1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b4585-3d9f-4036-a64d-c91e51c592ec",
   "metadata": {},
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "5984c5b4-6b51-48c3-bf68-7e841be1866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_K(kernel, x, eta):\n",
    "    N = len(x)\n",
    "    M = np.zeros((N, N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            M[i,j] = kernel(x[i], x[j], eta)\n",
    "            \n",
    "    K = M + M.T\n",
    "    np.fill_diagonal(K, 1)\n",
    "    \n",
    "    return(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "fe67cf23-0de5-4159-b59d-8d700feb712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) todo: implement this\n",
    "def negLogLikelihood(params, kernel):\n",
    "    noise_y = params[0]\n",
    "    eta = params[1:]\n",
    "    # todo: calculate the negative loglikelihood (See section 6.3 in the lecture notes)\n",
    "    \n",
    "    # Compute the kernel matrix K\n",
    "    K = compute_K(kernel, X, eta)\n",
    "    #K = kernel(X, X, eta)\n",
    "\n",
    "    # Add noise variance to the diagonal\n",
    "    K_y = K + noise_y**2 * np.eye(len(X))\n",
    "\n",
    "    # Compute the log determinant and the inverse of K_y\n",
    "    log_det = np.log(np.linalg.det(K_y))\n",
    "    K_y_inv = np.linalg.inv(K_y)\n",
    "\n",
    "    # Calculate the negative log likelihood\n",
    "    nll = 0.5 * log_det + 0.5 * np.dot(y.T, np.dot(K_y_inv, y)) + 0.5 * len(X) * np.log(2 * np.pi)\n",
    "    \n",
    "    return nll\n",
    "\n",
    "# B) todo: implement the posterior distribution, i.e. the distribution of f^star\n",
    "def conditional(X, y, noise_var, eta, kernel):\n",
    "    # todo: Write the function...\n",
    "    # See eq. 66 in the lecture notes. Note that there is a small error: Instead of (S) it should be K(S)\n",
    "    \n",
    "    K = compute_K(kernel, X, eta)\n",
    "    #K = kernel(X, y, eta)\n",
    "    \n",
    "    K_star = kernel(X, y, eta)\n",
    "    K_star_star = kernel(y, y, eta)\n",
    "\n",
    "    # Add noise variance to the diagonal of K\n",
    "    K_y = K + noise_var**2 * np.eye(len(X)) # K(S) + sima^2_y*I\n",
    "    K_y_inv = np.linalg.inv(K_y) # Compute the inverse of K_y\n",
    "\n",
    "    # Compute the mean of the predictive distribution\n",
    "    mustar = np.dot(K_star.T, np.dot(K_y_inv, y))\n",
    "\n",
    "    # Compute the covariance matrix of the predictive distribution\n",
    "    Sigmastar = K_star_star - np.dot(K_star, np.dot(K_y_inv, K_star.T))\n",
    "\n",
    "    return mustar, Sigmastar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b0656a14-0dcf-46fc-a61a-48c769db4b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[260], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m noise_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      4\u001b[0m eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m----> 5\u001b[0m prediction_mean_gp, Sigma_gp \u001b[38;5;241m=\u001b[39m \u001b[43mconditional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m var_gp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(Sigma_gp) \u001b[38;5;66;03m# We only need the diagonal term of the covariance matrix for the plots.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#plotting code for your convenience\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[259], line 28\u001b[0m, in \u001b[0;36mconditional\u001b[0;34m(X, y, noise_var, eta, kernel)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconditional\u001b[39m(X, y, noise_var, eta, kernel):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# todo: Write the function...\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# See eq. 66 in the lecture notes. Note that there is a small error: Instead of (S) it should be K(S)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_K\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#K = kernel(X, y, eta)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     K_star \u001b[38;5;241m=\u001b[39m kernel(X, y, eta)\n",
      "Cell \u001b[0;32mIn[258], line 7\u001b[0m, in \u001b[0;36mcompute_K\u001b[0;34m(kernel, x, eta)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m----> 7\u001b[0m         M[i,j] \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m K \u001b[38;5;241m=\u001b[39m M \u001b[38;5;241m+\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mfill_diagonal(K, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[235], line 2\u001b[0m, in \u001b[0;36mgaussian_kernel\u001b[0;34m(X, Xprime, gamma)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgaussian_kernel\u001b[39m(X, Xprime, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     dists \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msqeuclidean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mgamma \u001b[38;5;241m*\u001b[39m dists)\n",
      "File \u001b[0;32m~/.miniconda/envs/pyro/lib/python3.10/site-packages/scipy/spatial/distance.py:2982\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m sB \u001b[38;5;241m=\u001b[39m XB\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2982\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sB) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXB must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "# B) todo: use the learned GP to predict on the observations at X_predict\n",
    "kernel = gaussian_kernel\n",
    "noise_var = 2\n",
    "eta = 0.1\n",
    "prediction_mean_gp, Sigma_gp = conditional(X, y, noise_var, eta, kernel)\n",
    "var_gp = np.diag(Sigma_gp) # We only need the diagonal term of the covariance matrix for the plots.\n",
    "\n",
    "#plotting code for your convenience\n",
    "plt.figure(dpi=400,figsize=(6,3))\n",
    "plt.plot(X + 1958, y_raw, color='blue', label='training data')\n",
    "plt.plot(X_predict + 1958, y_predict, color='red', label='test data')\n",
    "\n",
    "yout_m = prediction_mean_gp * y_std + y_mean\n",
    "yout_v = var_gp*y_std**2\n",
    "\n",
    "plt.plot(X_predict + 1958, yout_m, color='black', label='gp prediction')\n",
    "plt.plot(X_predict + 1958, yout_m+1.96*yout_v**0.5, color='grey', label='GP uncertainty')\n",
    "plt.plot(X_predict + 1958, yout_m-1.96*yout_v**0.5, color='grey')\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"co2(ppm)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9c4cb-fdec-47ef-915a-15837179de2d",
   "metadata": {},
   "source": [
    "# c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "608e3d06-93a0-4c4f-9000-697aca0571bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_params(ranges, kernel, Ngrid):\n",
    "    opt_params = opt.brute(lambda params: negLogLikelihood(params, kernel), ranges, Ns=Ngrid, finish=None)\n",
    "    \n",
    "    noise_var = opt_params[0]\n",
    "    eta = opt_params[1:]\n",
    "    \n",
    "    return noise_var, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2d3d7beb-7467-4c63-9027-934e12fe022b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[257], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m ranges \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m1.e-4\u001b[39m,\u001b[38;5;241m10\u001b[39m), (\u001b[38;5;241m1.e-4\u001b[39m,\u001b[38;5;241m10\u001b[39m)) \u001b[38;5;66;03m# todo: change to the new parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m Ngrid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 6\u001b[0m noise_var, eta \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mranges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNgrid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimal params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, noise_var, eta)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# B) todo: use the learned GP to predict on the observations at X_predict\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m, in \u001b[0;36moptimize_params\u001b[0;34m(ranges, kernel, Ngrid)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_params\u001b[39m(ranges, kernel, Ngrid):\n\u001b[0;32m----> 2\u001b[0m     opt_params \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbrute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegLogLikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinish\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     noise_var \u001b[38;5;241m=\u001b[39m opt_params[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     eta \u001b[38;5;241m=\u001b[39m opt_params[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.miniconda/envs/pyro/lib/python3.10/site-packages/scipy/optimize/_optimize.py:3814\u001b[0m, in \u001b[0;36mbrute\u001b[0;34m(func, ranges, args, Ns, full_output, finish, disp, workers)\u001b[0m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# iterate over input arrays, possibly in parallel\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m MapWrapper(pool\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m mapper:\n\u001b[0;32m-> 3814\u001b[0m     Jout \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   3816\u001b[0m         grid \u001b[38;5;241m=\u001b[39m (grid,)\n",
      "File \u001b[0;32m~/.miniconda/envs/pyro/lib/python3.10/site-packages/scipy/optimize/_optimize.py:3887\u001b[0m, in \u001b[0;36m_Brute_Wrapper.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   3885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   3886\u001b[0m     \u001b[38;5;66;03m# flatten needed for one dimensional case.\u001b[39;00m\n\u001b[0;32m-> 3887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m, in \u001b[0;36moptimize_params.<locals>.<lambda>\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_params\u001b[39m(ranges, kernel, Ngrid):\n\u001b[0;32m----> 2\u001b[0m     opt_params \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mbrute(\u001b[38;5;28;01mlambda\u001b[39;00m params: \u001b[43mnegLogLikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m, ranges, Ns\u001b[38;5;241m=\u001b[39mNgrid, finish\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     noise_var \u001b[38;5;241m=\u001b[39m opt_params[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     eta \u001b[38;5;241m=\u001b[39m opt_params[\u001b[38;5;241m1\u001b[39m:]\n",
      "Cell \u001b[0;32mIn[215], line 8\u001b[0m, in \u001b[0;36mnegLogLikelihood\u001b[0;34m(params, kernel)\u001b[0m\n\u001b[1;32m      4\u001b[0m eta \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# todo: calculate the negative loglikelihood (See section 6.3 in the lecture notes)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the kernel matrix K\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_K\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming kernel is a function that computes the covariance matrix\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#K = kernel(X, X, eta)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add noise variance to the diagonal\u001b[39;00m\n\u001b[1;32m     12\u001b[0m K_y \u001b[38;5;241m=\u001b[39m K \u001b[38;5;241m+\u001b[39m noise_y\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mlen\u001b[39m(X))\n",
      "Cell \u001b[0;32mIn[209], line 7\u001b[0m, in \u001b[0;36mcompute_K\u001b[0;34m(kernel, x, eta)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,N):\n\u001b[0;32m----> 7\u001b[0m         M[i,j] \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m K \u001b[38;5;241m=\u001b[39m M \u001b[38;5;241m+\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mfill_diagonal(K, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[235], line 8\u001b[0m, in \u001b[0;36mspecial_kernel\u001b[0;34m(X, Xprime, eta)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspecial_kernel\u001b[39m(X, Xprime, eta):\n\u001b[1;32m      7\u001b[0m     a \u001b[38;5;241m=\u001b[39m eta[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[43meta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m     K \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mX\u001b[38;5;129m@Xprime\u001b[39m\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m a \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply\u001b[38;5;241m.\u001b[39mouter(np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mX\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39mb),np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mXprime\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39mb))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# C) todo: adapt this\n",
    "kernel = special_kernel # todo: change to new kernel\n",
    "ranges = ((1.e-4,10), (1.e-4,10)) # todo: change to the new parameters\n",
    "\n",
    "Ngrid = 10\n",
    "noise_var, eta = optimize_params(ranges, kernel, Ngrid)\n",
    "print(\"optimal params:\", noise_var, eta)\n",
    "\n",
    "# B) todo: use the learned GP to predict on the observations at X_predict\n",
    "prediction_mean_gp, Sigma_gp = conditional(X, y, noise_var, eta, kernel)\n",
    "var_gp = np.diag(Sigma_gp) # We only need the diagonal term of the covariance matrix for the plots.\n",
    "\n",
    "#plotting code for your convenience\n",
    "plt.figure(dpi=400,figsize=(6,3))\n",
    "plt.plot(X + 1958, y_raw, color='blue', label='training data')\n",
    "plt.plot(X_predict + 1958, y_predict, color='red', label='test data')\n",
    "\n",
    "yout_m = prediction_mean_gp * y_std + y_mean\n",
    "yout_v = var_gp*y_std**2\n",
    "\n",
    "plt.plot(X_predict + 1958, yout_m, color='black', label='gp prediction')\n",
    "plt.plot(X_predict + 1958, yout_m+1.96*yout_v**0.5, color='grey', label='GP uncertainty')\n",
    "plt.plot(X_predict + 1958, yout_m-1.96*yout_v**0.5, color='grey')\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"co2(ppm)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4479e-2bfc-4181-b4b5-7a6679bf71f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
